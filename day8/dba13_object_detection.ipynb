{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 객체탐지\n",
    "\n",
    "#### 개요\n",
    "- 딥러닝의 CNN(외 RCNN 등)와 같은 알고리즘을 통해서 물체를 인식하여 표시하는 기술\n",
    "- 자동차번호판 번호 인식, 화재경보, 교통사고인지, 이상행동파악 등...\n",
    "- CCTV과 같이 접목해서 활용되는 경우가 아주 많음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요 라이브러리\n",
    "- OpenCV - 최초 인텔에서 개발한 오픈소스 실시간 컴퓨터 비전 라이브러리\n",
    "    - C/C++을 목표로 제작. 크로스 플랫폼    \n",
    "    - 파이썬에 OpenCV가 적용되면서 활성화!\n",
    "    - 카메라 인식 산업에서 대부분 사용되고 있음\n",
    "    - C/C++에서 기본 동작코드 2~300줄이면 파이썬에선 10줄이내로 같은 작업을 할 수 있음\n",
    "\n",
    "- YOLO(PyTorch)\n",
    "    - Not You Only Live Once, You Only Look Once! \n",
    "    - 손쉽게 사용할 수 있는 실시간 객체 탐시 시스템\n",
    "    - 2015년에 출시후 현재 2024년 현재 v8.0 \n",
    "    - OpenCV만 가지고 작업하던 걸, YOLO로 넘어가는 추세"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from opencv-python) (1.26.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Window, Mac 차이가 없음\n",
    "## Raspbarry Pi는 최선버전에서 사용법이 변경되었음.\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이미지 로드\n",
    "## 사막여우 == Fennec Fox\n",
    "img = cv2.imread('./fennec_fox.png')\n",
    "\n",
    "cv2.imshow('Fox', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 현재 웹캠이 동작 안함\n",
    "video_path = './Mumbai_traffic.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path) # 0~숫자는 카메라번호\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "\n",
    "while (cap.isOpened()):  ## True => (cap.isOpened())\n",
    "    ret, img = cap.read() # 실시간으로 화면을 캡쳐 ret(결과정보객체), img(실시간이미지)\n",
    "    if ret == True:\n",
    "        cv2.imshow('youtube mpeg', img) ## 내부적으로 PyQt로 생성되는 GUI창\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'): # 키보드 q를 클릭하면\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release() # 자원 해제\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 이미지 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./fennec_fox.png')\n",
    "\n",
    "cv2.imshow('Original', img) ## 일반 이미지\n",
    "# cv2.waitKey(0) \n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "height, width = img.shape[0], img.shape[1]\n",
    "## 정수입력 width/2 => float 문제\n",
    "half_img = cv2.resize(gray, (int(width/2), int(height/2)))\n",
    "# cv2.imshow('Gray', gray) ## 흑백 변환\n",
    "cv2.imshow('half', half_img)\n",
    "cv2.waitKey(0) \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = './Mumbai_traffic.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path) # 0~숫자는 카메라번호\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "\n",
    "while (cap.isOpened()):   \n",
    "    ret, img = cap.read() # 실시간으로 화면을 캡쳐 ret(결과정보객체) 보통 사용하지 않아서 _로 변경, img(실시간이미지)\n",
    "    if ret == True:\n",
    "        # cv2.imshow('youtube mpeg', img) ## 내부적으로 PyQt로 생성되는 GUI창\n",
    "        height, width = img.shape[0], img.shape[1]\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  ## 컬러 -> 흑백으로\n",
    "        half = cv2.resize(gray, (int(width/2), int(height/2))) ## 사이즈를 반으로 축소\n",
    "        cv2.imshow('youtube gray', half)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'): # 키보드 q를 클릭하면\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release() # 자원 해제\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 포토샵등의 이미지, 프리미어등의 동영상 처리하는 프로그램에서 사용하는 거의 대부분의 기능이 OpenCV에 포함되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = './mbc_news.mp4'\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(video_path) # 0~숫자는 카메라번호\n",
    "\n",
    "while (cap.isOpened()):    \n",
    "    ret, img = cap.read() # 실시간으로 화면을 캡쳐 ret(결과정보객체) 보통 사용하지 않아서 _로 변경, img(실시간이미지)\n",
    "    if ret == True:\n",
    "        height, width = img.shape[0], img.shape[1]\n",
    "\n",
    "        half = cv2.resize(img, (int(width/2), int(height/2)))\n",
    "\n",
    "        # 얼굴인식\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            half,\n",
    "            scaleFactor=2.0,\n",
    "            minNeighbors=5,\n",
    "            minSize=(10,10)\n",
    "        )\n",
    "        ## 찾은 얼굴 위치 표시\n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(half,(x,y),(x+w,y+h),(0,255,255), 2)\n",
    "            roi_color = half[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.imshow('youtube mpeg', half) ## 내부적으로 PyQt로 생성되는 GUI창\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'): # 키보드 q를 클릭하면\n",
    "            break\n",
    "    \n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release() # 자원 해제\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### YOLO\n",
    "- You Only Look Once - CNN을 기반으로 한 물체 감지 라이브러리\n",
    "- https://www.ultralytics.com/ko\n",
    "- https://github.com/ultralytics/ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (8.2.76)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (1.26.3)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (3.9.1.post1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (2.4.0+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (0.19.0+cu121)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from ultralytics) (2.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
      "Requirement already satisfied: filelock in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: colorama in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\sources\\iot-bigdata-2024\\gpu_env\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# YOLO 설치\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n",
      "Ultralytics YOLOv8.2.76 🚀 Python-3.11.5 torch-2.4.0+cu121 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 c:\\Sources\\iot-bigdata-2024\\day8\\bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 28.5ms\n",
      "Speed: 3.5ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mC:\\Sources\\iot-bigdata-2024\\runs\\detect\\predict2\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/6.25M [00:00<?, ?B/s]\n",
      "  2%|▏         | 128k/6.25M [00:00<00:05, 1.07MB/s]\n",
      "  8%|▊         | 512k/6.25M [00:00<00:02, 2.13MB/s]\n",
      " 12%|█▏        | 768k/6.25M [00:00<00:02, 2.29MB/s]\n",
      " 20%|██        | 1.25M/6.25M [00:00<00:02, 2.52MB/s]\n",
      " 28%|██▊       | 1.75M/6.25M [00:00<00:01, 3.16MB/s]\n",
      " 42%|████▏     | 2.62M/6.25M [00:00<00:00, 4.74MB/s]\n",
      " 56%|█████▌    | 3.50M/6.25M [00:00<00:00, 5.07MB/s]\n",
      " 66%|██████▌   | 4.12M/6.25M [00:01<00:00, 5.34MB/s]\n",
      " 84%|████████▍ | 5.25M/6.25M [00:01<00:00, 7.01MB/s]\n",
      " 96%|█████████▌| 6.00M/6.25M [00:01<00:00, 7.19MB/s]\n",
      "100%|██████████| 6.25M/6.25M [00:01<00:00, 5.11MB/s]\n"
     ]
    }
   ],
   "source": [
    "## 콘솔창에서 테스트하는 방법, 트레이닝한 이미지처리 모델\n",
    "!yolo predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Sources\\iot-bigdata-2024\\day8\\20190417_194709.jpg: 384x640 1 bottle, 1 cup, 1 bowl, 1 tv, 1 mouse, 31.3ms\n",
      "Speed: 5.0ms preprocess, 31.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# 이미지와 openCV 물체감지\n",
    "model = YOLO(model='./yolov8n.pt')\n",
    "\n",
    "result = model('./20190417_194709.jpg')\n",
    "plots = result[0].plot()\n",
    "height, width = plots.shape[0], plots.shape[1]\n",
    "last = cv2.resize(plots, (800, 450))\n",
    "cv2.imshow('yolo', last)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실시간 가능, 동영상도 가능\n",
    "classNames = [\n",
    "                \"person\", \"bicycle\", \"car\", \"motorbike\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "                \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "                \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "                \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "                \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "                \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "                \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "                \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "                \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "                \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Mumbai_traffic.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mVideoCapture(video_path) \u001b[38;5;66;03m# 숫자는 CCTV,웹캠 등 실시간 영상\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (cap\u001b[38;5;241m.\u001b[39misOpened()):\n\u001b[0;32m      6\u001b[0m     ret, img \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "video_path = './Mumbai_traffic.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path) # 숫자는 CCTV,웹캠 등 실시간 영상\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    ret, img = cap.read()\n",
    "    if ret == True:\n",
    "        height, width = img.shape[0], img.shape[1]\n",
    "        half = cv2.resize(img, (int(width/2), int(height/2)))\n",
    "        # YOLO로 물체검출 시작\n",
    "        results = model(half, stream=True)\n",
    "\n",
    "        ## 결과표시 like OpenCV 얼굴검출\n",
    "        for result in results:\n",
    "            ## 아래의 셀의 결과가 간단\n",
    "            boxes = result.boxes\n",
    "\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "                cv2.rectangle(half, (x1,y1), (x2,y2), (0,255,255), 2) # 검출된 물체 박스그리기\n",
    "\n",
    "                ## 정확도계산, Class name\n",
    "                accuracy = (box.conf[0]/100)*100\n",
    "                index = int(box.cls[0])\n",
    "                # print(f'ClassName : {classNames[index]} / Accuracy : {accuracy:.2f}') # 콘솔프린트는 생략\n",
    "                title = f'{classNames[index]}, {accuracy:.2f}'\n",
    "                ## 박스위에 종류와 정확도출력\n",
    "                org = [x1, y1] \n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                fontScale = 0.6\n",
    "                color = (0,255,255)\n",
    "                thickness = 2\n",
    "\n",
    "                cv2.putText(half, title, org, font, fontScale, color, thickness)\n",
    "\n",
    "        cv2.imshow('YOLOv8', half)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Mumbai_traffic.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mVideoCapture(video_path) \u001b[38;5;66;03m# 숫자는 CCTV,웹캠 등 실시간 영상\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (cap\u001b[38;5;241m.\u001b[39misOpened()):\n\u001b[0;32m      6\u001b[0m     ret, img \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "video_path = './Mumbai_traffic.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path) # 숫자는 CCTV,웹캠 등 실시간 영상\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    ret, img = cap.read()\n",
    "    if ret == True:\n",
    "        height, width = img.shape[0], img.shape[1]\n",
    "        half = cv2.resize(img, (int(width/2), int(height/2)))\n",
    "        results = model(half, stream=True)\n",
    "\n",
    "        for result in results:\n",
    "            last = result.plot()\n",
    "\n",
    "        cv2.imshow('YOLOv8', last)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m height, width \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     10\u001b[0m half \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (\u001b[38;5;28mint\u001b[39m(width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mint\u001b[39m(height\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(half, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     14\u001b[0m     last \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "video_path = './Mumbai_traffic.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "out = cv2.VideoWriter('./Mumbai_traffic_result.mp4', fourcc=1446269005, fps=30, frameSize=(640,360))\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    ret, img = cap.read()\n",
    "    if ret == True:\n",
    "        height, width = img.shape[0], img.shape[1]\n",
    "        half = cv2.resize(img, (int(width/2), int(height/2)))\n",
    "        results = model(half, stream=True)\n",
    "\n",
    "        for result in results:\n",
    "            last = result.plot()\n",
    "\n",
    "        cv2.imshow('YOLOv8', last)\n",
    "        out.write(last)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
